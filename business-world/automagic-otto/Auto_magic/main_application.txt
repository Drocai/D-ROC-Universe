#!/usr/bin/env python3
"""
AutoMagic (O.T.T.O.) - Autonomous Content Creation System

A fully automated AI-driven system for generating, assembling and publishing
surreal, affirmation-style short-form video content across multiple platforms.

This script implements the complete AutoMagic pipeline:
1. Trend analysis (sounds, visual styles, hashtags)
2. AI-driven content generation (text, audio, images, video)
3. Video assembly with trendy effects
4. Automated publishing to social platforms
5. Scheduling for daily execution

Created as part of the AutoMagic Project, 2025
"""

import os
import sys
import time
import json
import base64
import logging
import argparse
import random
import schedule
import requests
import numpy as np
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

# Import external libraries
from dotenv import load_dotenv
from PIL import Image, ImageDraw, ImageFont
from bs4 import BeautifulSoup
import openai
import elevenlabs
import google.generativeai as genai
from moviepy.editor import (
    VideoFileClip, ImageClip, AudioFileClip,
    CompositeAudioClip, CompositeVideoClip, concatenate_videoclips,
    TextClip, vfx, ColorClip, ImageSequenceClip
)
from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from googleapiclient.http import MediaFileUpload


# --- Configuration and Setup ---

def load_config():
    """
    Loads environment variables from .env file and configures API clients.
    Includes comprehensive error handling and validation for each configuration parameter.
    
    Returns:
        dict: Configuration dictionary with all settings and API credentials
    """
    # Load environment variables
    load_dotenv()
    
    # Initialize configuration dictionary
    config = {
        # OpenAI (DALL-E) configuration
        "openai_api_key": os.getenv("OPENAI_API_KEY"),
        
        # ElevenLabs (TTS) configuration
        "elevenlabs_api_key": os.getenv("ELEVENLABS_API_KEY"),
        "elevenlabs_voice_id": os.getenv("ELEVENLABS_VOICE_ID"),
        
        # Google services configuration
        "google_api_key": os.getenv("GOOGLE_API_KEY"),  # For Gemini/VEO
        "google_cloud_project": os.getenv("GOOGLE_CLOUD_PROJECT"),
        "youtube_client_id": os.getenv("YOUTUBE_CLIENT_ID"),
        "youtube_client_secret": os.getenv("YOUTUBE_CLIENT_SECRET"),
        "youtube_channel_id": os.getenv("YOUTUBE_CHANNEL_ID"),
        "google_api_credentials_file": os.getenv("GOOGLE_API_CREDENTIALS_FILE", "youtube_credentials.json"),
        
        # Third-party video generation APIs
        "kling_api_key": os.getenv("KLING_API_KEY"),
        "kling_api_endpoint": os.getenv("KLING_API_ENDPOINT"),
        "pictory_api_key": os.getenv("PICTORY_API_KEY"),
        
        # ComfyUI configuration
        "comfyui_api_address": os.getenv("COMFYUI_API_ADDRESS", "http://127.0.0.1:8188"),
        
        # TikTok configuration
        "tiktok_api_key": os.getenv("TIKTOK_API_KEY"),
        "tiktok_session_id": os.getenv("TIKTOK_SESSION_ID"),
        
        # AutoMagic settings
        "daily_run_time": os.getenv("DAILY_RUN_TIME", "09:00"),
        "log_file_path": os.getenv("LOG_FILE_PATH", "logs/automagic.log"),
        "image_save_path": os.getenv("IMAGE_SAVE_PATH", "generated_images/"),
        "audio_save_path": os.getenv("AUDIO_SAVE_PATH", "generated_audio/"),
        "video_clip_save_path": os.getenv("VIDEO_CLIP_SAVE_PATH", "generated_video_clips/"),
        "final_video_save_path": os.getenv("FINAL_VIDEO_SAVE_PATH", "final_videos/"),
        "season": int(os.getenv("SEASON", 1)),
        "day_number": int(os.getenv("DAY_NUMBER", 1)),
        "cleanup_temp_files": os.getenv("CLEANUP_TEMP_FILES", "false").lower() == "true",
        
        # Internal configuration
        "content_themes": {
            1: "Everyday Absurdity",  # Season 1 theme
            2: "Surreal Self-Care",   # Season 2 theme
            3: "Existential Encouragement"  # Season 3 theme
        },
        "missing_api_warnings": []  # Store warnings about missing APIs for later reporting
    }
    
    # Ensure all directory paths end with a slash
    for path_key in ["image_save_path", "audio_save_path", "video_clip_save_path", "final_video_save_path"]:
        if config[path_key] and not config[path_key].endswith('/'):
            config[path_key] += '/'
    
    # Create required directories with proper error handling
    required_dirs = [
        os.path.dirname(config["log_file_path"]), 
        config["image_save_path"],
        config["audio_save_path"], 
        config["video_clip_save_path"],
        config["final_video_save_path"]
    ]
    
    for directory in required_dirs:
        try:
            os.makedirs(directory, exist_ok=True)
            logging.debug(f"Directory ensured: {directory}")
        except Exception as e:
            logging.error(f"Failed to create directory {directory}: {str(e)}")
            raise RuntimeError(f"Critical error: Failed to create required directory {directory}") from e
    
    # Configure API clients with comprehensive error handling
    
    # 1. Configure Google Generative AI (Gemini/VEO)
    if config["google_api_key"]:
        try:
            genai.configure(api_key=config["google_api_key"])
            logging.info("Google Generative AI (Gemini) configured successfully")
            
            # Validate API key with a simple test
            try:
                model = genai.GenerativeModel('gemini-1.5-flash')
                response = model.generate_content("Test")
                if response:
                    logging.info("Google Gemini API connection verified")
            except Exception as e:
                logging.warning(f"Google Gemini API key validation failed: {str(e)}")
                config["missing_api_warnings"].append("Google Gemini API key appears invalid")
                
        except Exception as e:
            logging.error(f"Failed to configure Google Generative AI: {str(e)}")
            config["missing_api_warnings"].append("Google Generative AI configuration failed")
    else:
        logging.warning("Google API Key not found in .env file. Gemini/VEO features will not work.")
        config["missing_api_warnings"].append("Google API Key missing")

    # 2. Configure OpenAI
    if config["openai_api_key"]:
        try:
            # Configure the client
            openai.api_key = config["openai_api_key"]
            logging.info("OpenAI configured successfully")
        except Exception as e:
            logging.error(f"Failed to configure OpenAI: {str(e)}")
            config["missing_api_warnings"].append("OpenAI configuration failed")
    else:
        logging.warning("OpenAI API Key not found in .env file. DALL-E features will not work.")
        config["missing_api_warnings"].append("OpenAI API Key missing")

    # 3. Configure ElevenLabs
    if config["elevenlabs_api_key"]:
        try:
            elevenlabs.set_api_key(config["elevenlabs_api_key"])
            logging.info("ElevenLabs configured successfully")
            
            # Validate voice ID is available
            if not config["elevenlabs_voice_id"]:
                logging.warning("ElevenLabs voice ID not specified. Will use default voice.")
        except Exception as e:
            logging.error(f"Failed to configure ElevenLabs: {str(e)}")
            config["missing_api_warnings"].append("ElevenLabs configuration failed")
    else:
        logging.warning("ElevenLabs API Key not found in .env file. TTS features will not work.")
        config["missing_api_warnings"].append("ElevenLabs API Key missing")
    
    # 4. Validate ComfyUI server availability if address is provided
    if config["comfyui_api_address"]:
        try:
            response = requests.get(f"{config['comfyui_api_address']}/system_stats", timeout=5)
            if response.status_code == 200:
                logging.info("ComfyUI server is accessible")
            else:
                logging.warning(f"ComfyUI server returned status code {response.status_code}")
                config["missing_api_warnings"].append("ComfyUI server connection issues")
        except requests.exceptions.RequestException as e:
            logging.warning(f"ComfyUI server connection failed: {str(e)}")
            config["missing_api_warnings"].append("ComfyUI server connection failed")
    
    # 5. Check YouTube API credentials file exists if specified
    client_secrets_file = "client_secret.json"
    if os.path.exists(client_secrets_file):
        logging.info(f"YouTube client secrets file found: {client_secrets_file}")
        
        # Optionally validate content structure
        try:
            with open(client_secrets_file, 'r') as f:
                secrets = json.load(f)
            if "web" in secrets or "installed" in secrets:
                logging.info("YouTube client secrets file appears valid")
            else:
                logging.warning("YouTube client secrets file may be invalid: missing expected fields")
                config["missing_api_warnings"].append("YouTube client secrets file may be invalid")
        except Exception as e:
            logging.warning(f"Failed to validate YouTube client secrets file: {str(e)}")
            config["missing_api_warnings"].append("YouTube client secrets file validation failed")
    else:
        logging.warning(f"YouTube client secrets file not found: {client_secrets_file}")
        config["missing_api_warnings"].append("YouTube client secrets file missing")
    
    # Report all configuration issues at once
    if config["missing_api_warnings"]:
        logging.warning("Configuration issues detected: " + ", ".join(config["missing_api_warnings"]))
    else:
        logging.info("All API configurations validated successfully")
        
    return config


def setup_logging(log_file_path):
    """
    Sets up comprehensive logging with rotation, formatting and multiple handlers.
    
    Args:
        log_file_path (str): Path to the log file
    """
    # Ensure log directory exists
    log_dir = os.path.dirname(log_file_path)
    os.makedirs(log_dir, exist_ok=True)
    
    # Set up custom formatter with timestamp, level, and message
    formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(filename)s:%(lineno)d - %(funcName)s - %(message)s'
    )
    
    # Create file handler with rotation
    try:
        # Configure file handler
        file_handler = logging.FileHandler(log_file_path)
        file_handler.setFormatter(formatter)
        
        # Configure console handler
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        
        # Configure root logger
        root_logger = logging.getLogger()
        root_logger.setLevel(logging.INFO)  # Set default level
        root_logger.addHandler(file_handler)
        root_logger.addHandler(console_handler)
        
        # Reduce verbosity of some third-party libraries
        logging.getLogger('urllib3').setLevel(logging.WARNING)
        logging.getLogger('PIL').setLevel(logging.WARNING)
        logging.getLogger('moviepy').setLevel(logging.WARNING)
        
        logging.info("Logging setup complete")
    except Exception as e:
        # Fallback to basic configuration if advanced setup fails
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[logging.StreamHandler()]
        )
        logging.error(f"Failed to set up file logging to {log_file_path}: {str(e)}")
        logging.info("Fallback logging configuration activated")


def load_project_state():
    """
    Loads project state data from a persisted file.
    This function maintains the day/season counters between runs.
    
    Returns:
        dict: Current project state data
    """
    state_file = "project_state.json"
    default_state = {
        "season": 1,
        "day_number": 1,
        "completed_videos": [],
        "last_run": None
    }
    
    try:
        if os.path.exists(state_file):
            with open(state_file, 'r') as f:
                state = json.load(f)
            logging.info(f"Loaded project state: Season {state['season']}, Day {state['day_number']}")
            return state
        else:
            logging.info("No previous state found, using defaults")
            return default_state
    except Exception as e:
        logging.error(f"Error loading project state: {str(e)}")
        logging.info("Using default project state")
        return default_state


def save_project_state(state):
    """
    Persists project state data to a file.
    
    Args:
        state (dict): Current project state to save
    """
    state_file = "project_state.json"
    try:
        with open(state_file, 'w') as f:
            json.dump(state, f, indent=2)
        logging.info(f"Project state saved: Season {state['season']}, Day {state['day_number']}")
    except Exception as e:
        logging.error(f"Failed to save project state: {str(e)}")


# --- Content Generation Functions ---

def get_trending_data():
    """
    Retrieves trending sounds, styles, hashtags from platforms like TikTok or YouTube.
    Uses a combination of API calls and web scraping with fallback mechanisms.
    
    Returns:
        tuple: (trending_sound_path, trending_hashtags, trending_visual_style)
    """
    logging.info("Fetching trending data...")
    
    # Initialize default/fallback values in case API calls fail
    trending_sounds = []
    trending_hashtags = []
    trending_visual_styles = []
    
    # Method 1: TikTok API/scraping approach
    try:
        # TikTok trending hashtags via scraping (since official API has limitations)
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get('https://www.tiktok.com/discover', headers=headers, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            # Extract trending hashtags based on DOM structure
            # Note: This is a simplified approach and may need adjustment as TikTok's structure changes
            hashtag_elements = soup.select('.tiktok-trending-hashtag')  # Placeholder selector
            if hashtag_elements:
                for element in hashtag_elements[:10]:  # Get top 10
                    hashtag = element.text.strip()
                    if hashtag.startswith('#'):
                        trending_hashtags.append(hashtag)
            
            logging.info(f"Retrieved {len(trending_hashtags)} trending hashtags from TikTok")
        else:
            logging.warning(f"TikTok trending data request failed with status code: {response.status_code}")
    except Exception as e:
        logging.error(f"Error retrieving TikTok trending data: {str(e)}")
    
    # Method 2: YouTube Trending via Data API (if credentials available)
    try:
        # This would use youtube_service from the youtube upload function
        # Not implementing full API call here to avoid complexity
        # youtube_trends = youtube_service.videos().list(
        #     part="snippet",
        #     chart="mostPopular",
        #     regionCode="US",
        #     maxResults=10
        # ).execute()
        pass
    except Exception as e:
        logging.error(f"Error retrieving YouTube trending data: {str(e)}")
    
    # Fallback to curated trending data if API methods fail
    if not trending_hashtags:
        logging.info("Using curated trending hashtags as fallback")
        trending_hashtags = [
            "#AIcontent", "#WeirdCore", "#Liminal", "#Surreal", "#AltReality",
            "#Affirmation", "#AIart", "#DigitalDystopia", "#TechSurrealism",
            "#MindBending", "#GlitchArt", "#VaporwaveAesthetic", "#SurrealMemes"
        ]
    
    # Trending visual styles (curated list based on trends)
    trending_visual_styles = [
        "vaporwave", "glitchcore", "weirdcore", "liminal", "cyberpunk", 
        "y2k", "dreamcore", "dystopian", "synthwave", "retrofuturism"
    ]
    
    # Trending sounds - local library approach
    # In a real implementation, we would download trending sounds from platforms
    # For now, we'll use a local library of sounds
    sound_library_dir = "assets/sounds/"
    os.makedirs(sound_library_dir, exist_ok=True)
    
    # Check if there are any sounds in the library
    sound_files = []
    if os.path.exists(sound_library_dir):
        sound_files = [f for f in os.listdir(sound_library_dir) if f.endswith(('.mp3', '.wav'))]
    
    # If no sounds found, create a placeholder
    if not sound_files:
        placeholder_sound_path = os.path.join(sound_library_dir, "placeholder_beat.mp3")
        # In a real implementation, would download a royalty-free sound
        # For now, just log that we would need to add sounds to the library
        logging.warning(f"No sounds found in library. Please add sound files to {sound_library_dir}")
        return None, trending_hashtags, random.choice(trending_visual_styles)
    else:
        # Select a random sound from the library
        selected_sound = random.choice(sound_files)
        trending_sound_path = os.path.join(sound_library_dir, selected_sound)
        logging.info(f"Selected trending sound: {trending_sound_path}")
        
    # Return the trending data triple
    return trending_sound_path, trending_hashtags, random.choice(trending_visual_styles)


def generate_llm_text(prompt, config, temperature=0.7, max_retries=3):
    """
    Generates text using Gemini or fallback LLM model with retry mechanism.
    
    Args:
        prompt (str): The prompt to send to the LLM
        config (dict): Application configuration
        temperature (float): Creativity level (0.0-1.0)
        max_retries (int): Maximum number of retry attempts
        
    Returns:
        str: Generated text or None if all attempts fail
    """
    logging.info(f"Generating LLM text with prompt: {prompt[:100]}...")
    
    retry_count = 0
    while retry_count < max_retries:
        try:
            # Check if Google Gemini API is available
            if config["google_api_key"]:
                # Use Gemini for text generation
                model = genai.GenerativeModel('gemini-1.5-flash')
                generation_config = {
                    "temperature": temperature,
                    "top_p": 0.95,
                    "top_k": 40,
                    "max_output_tokens": 2048,
                }
                
                response = model.generate_content(
                    prompt,
                    generation_config=generation_config
                )
                
                if hasattr(response, 'text'):
                    generated_text = response.text.strip()
                    logging.info(f"Successfully generated text with Gemini ({len(generated_text)} chars)")
                    return generated_text
                else:
                    logging.warning("Gemini returned an empty or invalid response")
                    retry_count += 1
                    time.sleep(2)  # Add delay before retry
            
            # Fallback to OpenAI if Gemini fails or is not configured
            elif config["openai_api_key"]:
                logging.info("Falling back to OpenAI for text generation")
                import openai
                
                response = openai.chat.completions.create(
                    model="gpt-4-turbo",
                    messages=[{"role": "user", "content": prompt}],
                    temperature=temperature,
                    max_tokens=2048
                )
                
                if response.choices and response.choices[0].message.content:
                    generated_text = response.choices[0].message.content.strip()
                    logging.info(f"Successfully generated text with OpenAI ({len(generated_text)} chars)")
                    return generated_text
                else:
                    logging.warning("OpenAI returned an empty or invalid response")
                    retry_count += 1
                    time.sleep(2)  # Add delay before retry
            
            else:
                # Both APIs are unavailable, use fallback text
                logging.warning("No LLM APIs available. Using fallback text generation.")
                
                # Generate basic fallback content based on the prompt
                prompt_keywords = prompt.lower()
                
                # Extract theme or topic from prompt
                if "generate" in prompt_keywords and "affirmation" in prompt_keywords:
                    theme = prompt.split("for:")[-1].strip() if "for:" in prompt else "surreal existence"
                    affirmations = [
                        f"Your reality is just a suggestion. Ignore it when convenient.",
                        f"When the void stares back, offer it a sandwich.",
                        f"You are not your thoughts. You are the cosmic background radiation.",
                        f"Time is an illusion. Lunchtime doubly so.",
                        f"The universe is made of stories, not atoms. Rewrite yours daily.",
                        f"Your consciousness is just visiting this dimension. Be a good tourist.",
                        f"Reality glitches are not errors. They're features of your unique simulation."
                    ]
                    return random.choice(affirmations)
                else:
                    return f"The boundaries between dreams and reality are more permeable than they appear. Step through."
        
        except Exception as e:
            logging.error(f"Error in LLM text generation (attempt {retry_count+1}/{max_retries}): {str(e)}")
            retry_count += 1
            time.sleep(2)  # Add delay before retry
    
    # All attempts failed, return a generic response
    logging.error("All LLM text generation attempts failed. Using emergency fallback text.")
    return "The universe speaks in riddles. Sometimes the answer is to stop looking for one."


def generate_tts_audio(text, voice_id, save_path, config, max_retries=3):
    """
    Generates text-to-speech audio using ElevenLabs API with fallback mechanisms.
    
    Args:
        text (str): Text to convert to speech
        voice_id (str): ElevenLabs voice ID
        save_path (str): Directory to save the audio file
        config (dict): Application configuration
        max_retries (int): Maximum number of retry attempts
        
    Returns:
        str: Path to generated audio file or None if failed
    """
    logging.info(f"Generating TTS audio for text: {text[:100]}...")
    
    if not text:
        logging.error("Empty text provided for TTS generation")
        return None
    
    # Create timestamp for unique filename
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    file_path = os.path.join(save_path, f"tts_{timestamp}.mp3")
    
    retry_count = 0
    while retry_count < max_retries:
        # Use ElevenLabs if available
        if config["elevenlabs_api_key"]:
            try:
                # Use specified voice or default
                voice = voice_id if voice_id else "premade/Adam"
                
                # Generate audio with ElevenLabs
                audio = elevenlabs.generate(
                    text=text,
                    voice=voice,
                    model="eleven_turbo_v2"
                )
                
                # Save the audio file
                elevenlabs.save(audio, file_path)
                
                # Verify file exists and has content
                if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
                    logging.info(f"TTS audio generated and saved to {file_path}")
                    return file_path
                else:
                    logging.warning(f"TTS audio file is empty or missing: {file_path}")
                    retry_count += 1
            
            except Exception as e:
                logging.error(f"Error in ElevenLabs TTS generation (attempt {retry_count+1}/{max_retries}): {str(e)}")
                retry_count += 1
                time.sleep(2)  # Add delay before retry
        
        # If ElevenLabs is not available or failed, try fallback approach
        else:
            logging.warning("ElevenLabs API not available. Attempting fallback TTS...")
            try:
                # Use Google Cloud Text-to-Speech if configured (simplified example)
                # In a real implementation, would integrate with a fallback TTS service
                # For now, create a dummy audio file
                logging.warning("Using placeholder audio due to missing TTS API")
                
                # Create a minimal dummy audio file
                with open(file_path, 'wb') as f:
                    # Write a minimal valid MP3 header
                    f.write(b'\xFF\xFB\x90\x44\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00')
                
                logging.info(f"Placeholder audio file created at {file_path}")
                return file_path
            
            except Exception as e:
                logging.error(f"Error in fallback TTS generation: {str(e)}")
                return None
    
    logging.error("All TTS generation attempts failed")
    return None


def generate_dalle_image(prompt, save_path, config, max_retries=3):
    """
    Generates an image using DALL-E API with retry mechanism and fallback options.
    
    Args:
        prompt (str): Image generation prompt
        save_path (str): Directory to save the image
        config (dict): Application configuration
        max_retries (int): Maximum number of retry attempts
        
    Returns:
        str: Path to generated image or None if failed
    """
    logging.info(f"Generating DALL-E image for prompt: {prompt[:100]}...")
    
    if not prompt:
        logging.error("Empty prompt provided for image generation")
        return None
        
    # Create timestamp for unique filename
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    file_path = os.path.join(save_path, f"dalle_{timestamp}.png")
    
    retry_count = 0
    while retry_count < max_retries:
        # Use OpenAI DALL-E if available
        if config["openai_api_key"]:
            try:
                import openai
                
                # Generate image with DALL-E
                response = openai.images.generate(
                    model="dall-e-3",
                    prompt=prompt,
                    n=1,
                    size="1024x1024",
                    quality="standard",
                    response_format="url"
                )
                
                # Download the image
                if response.data and response.data[0].url:
                    image_url = response.data[0].url
                    image_response = requests.get(image_url, stream=True, timeout=30)
                    image_response.raise_for_status()
                    
                    with open(file_path, 'wb') as f:
                        for chunk in image_response.iter_content(8192):
                            f.write(chunk)
                    
                    # Verify file exists and has content
                    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
                        logging.info(f"DALL-E image generated and saved to {file_path}")
                        return file_path
                    else:
                        logging.warning(f"DALL-E image file is empty or missing: {file_path}")
                        retry_count += 1
                else:
                    logging.warning("DALL-E API returned no image data")
                    retry_count += 1
            
            except Exception as e:
                logging.error(f"Error in DALL-E image generation (attempt {retry_count+1}/{max_retries}): {str(e)}")
                retry_count += 1
                time.sleep(2)  # Add delay before retry
        
        # Try ComfyUI if available as fallback
        elif config["comfyui_api_address"]:
            try:
                logging.info("Falling back to ComfyUI for image generation")
                comfyui_result = run_comfyui_workflow(prompt, save_path, config)
                if comfyui_result:
                    return comfyui_result
                else:
                    logging.warning("ComfyUI fallback failed")
                    retry_count += 1
            except Exception as e:
                logging.error(f"Error in ComfyUI fallback: {str(e)}")
                retry_count += 1
        
        # Last resort: generate a placeholder image
        else:
            logging.warning("No image generation APIs available. Creating placeholder image.")
            try:
                # Create a gradient placeholder image
                width, height = 1024, 1024
                image = Image.new('RGB', (width, height))
                
                # Generate a gradient based on the prompt to make it somewhat unique
                # Use hash of prompt to determine colors
                import hashlib
                prompt_hash = hashlib.md5(prompt.encode()).hexdigest()
                r = int(prompt_hash[:2], 16)
                g = int(prompt_hash[2:4], 16)
                b = int(prompt_hash[4:6], 16)
                
                pixels = image.load()
                for i in range(width):
                    for j in range(height):
                        # Create a gradient
                        pixels[i, j] = (
                            (r + i // 4) % 256,
                            (g + j // 4) % 256,
                            (b + (i + j) // 8) % 256
                        )
                
                # Add text label indicating this is a placeholder
                from PIL import ImageDraw, ImageFont
                draw = ImageDraw.Draw(image)
                try:
                    font = ImageFont.truetype("arial.ttf", 40)
                except Exception:
                    font = ImageFont.load_default()
                
                # Add prompt text as overlay
                draw.text((50, 50), "PLACEHOLDER IMAGE", fill=(255, 255, 255), font=font)
                draw.text((50, 100), prompt[:50] + "...", fill=(255, 255, 255), font=font)
                
                # Save the image
                image.save(file_path)
                
                logging.info(f"Placeholder image created at {file_path}")
                return file_path
            
            except Exception as e:
                logging.error(f"Error creating placeholder image: {str(e)}")
                # Create absolute minimal image if all else fails
                try:
                    Image.new('RGB', (100, 100), color=(0, 0, 0)).save(file_path)
                    return file_path
                except Exception:
                    return None
    
    logging.error("All image generation attempts failed")
    return None


def run_comfyui_workflow(prompt, save_path, config, max_retries=3):
    """
    Executes a ComfyUI workflow via API to generate images based on a text prompt.
    
    Args:
        prompt (str): Image generation prompt
        save_path (str): Directory to save the image
        config (dict): Application configuration
        max_retries (int): Maximum number of retry attempts
        
    Returns:
        str: Path to generated image or None if failed
    """
    logging.info(f"Running ComfyUI workflow for prompt: {prompt[:100]}...")
    
    if not config["comfyui_api_address"]:
        logging.error("ComfyUI API address not specified in configuration")
        return None
    
    # Create timestamp for unique filename
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    file_path = os.path.join(save_path, f"comfyui_{timestamp}.png")
    
    # Load a basic workflow template (simplified example)
    # In a real implementation, would load from a file or define programmatically
    # This is a simplified placeholder workflow JSON
    workflow = {
        "prompt": {
            # This is a simplified example - actual ComfyUI workflows are more complex
            "3": {
                "inputs": {
                    "text": prompt,
                    "clip": ["5", 0]
                },
                "class_type": "CLIPTextEncode"
            },
            "4": {
                "inputs": {
                    "width": 1024,
                    "height": 1024,
                    "batch_size": 1
                },
                "class_type": "EmptyLatentImage"
            },
            "5": {
                "inputs": {
                    "stop_at_clip_layer": -1,
                    "clip_name": "CLIP-ViT-H-14"
                },
                "class_type": "CLIPLoader"
            },
            "8": {
                "inputs": {
                    "samples": ["10", 0],
                    "vae": ["9", 0]
                },
                "class_type": "VAEDecode"
            },
            "9": {
                "inputs": {
                    "vae_name": "vae-ft-mse-840000"
                },
                "class_type": "VAELoader"
            },
            "10": {
                "inputs": {
                    "seed": random.randint(1, 2147483647),
                    "steps": 30,
                    "cfg": 8.0,
                    "sampler_name": "euler_ancestral",
                    "scheduler": "normal",
                    "denoise": 1.0,
                    "model": ["14", 0],
                    "positive": ["3", 0],
                    "negative": ["13", 0],
                    "latent_image": ["4", 0]
                },
                "class_type": "KSampler"
            },
            "13": {
                "inputs": {
                    "text": "low quality, bad anatomy, worst quality, low resolution",
                    "clip": ["5", 0]
                },
                "class_type": "CLIPTextEncode"
            },
            "14": {
                "inputs": {
                    "model_name": "stabilityai/stable-diffusion-xl-base-1.0"
                },
                "class_type": "ModelLoader"
            },
            "15": {
                "inputs": {
                    "filename_prefix": "ComfyUI",
                    "images": ["8", 0]
                },
                "class_type": "SaveImage"
            }
        }
    }
    
    retry_count = 0
    while retry_count < max_retries:
        try:
            # 1. Check if ComfyUI server is available
            api_base = config["comfyui_api_address"].rstrip('/')
            response = requests.get(f"{api_base}/system_stats", timeout=5)
            response.raise_for_status()
            
            # 2. Submit the workflow to ComfyUI
            queue_response = requests.post(f"{api_base}/prompt", json=workflow)
            queue_response.raise_for_status()
            prompt_id = queue_response.json().get('prompt_id')
            
            if not prompt_id:
                logging.error("ComfyUI did not return a prompt ID")
                retry_count += 1
                continue
            
            # 3. Poll for workflow completion
            for _ in range(60):  # Wait up to 60 seconds
                history_response = requests.get(f"{api_base}/history/{prompt_id}")
                if history_response.status_code == 200:
                    history = history_response.json()
                    if history.get(prompt_id, {}).get('outputs'):
                        # Images are usually in outputs > node_id > images
                        for node_id, output in history[prompt_id].get('outputs', {}).items():
                            if 'images' in output:
                                for img_data in output['images']:
                                    image_filename = img_data.get('filename')
                                    if image_filename:
                                        # Download the image
                                        image_url = f"{api_base}/view?filename={image_filename}&subfolder=&type=output"
                                        img_response = requests.get(image_url, stream=True)
                                        img_response.raise_for_status()
                                        
                                        with open(file_path, 'wb') as f:
                                            for chunk in img_response.iter_content(8192):
                                                f.write(chunk)
                                        
                                        if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
                                            logging.info(f"ComfyUI image generated and saved to {file_path}")
                                            return file_path
                
                time.sleep(1)  # Poll every second
            
            logging.warning("ComfyUI workflow timed out")
            retry_count += 1
        
        except requests.exceptions.RequestException as e:
            logging.error(f"ComfyUI API request error (attempt {retry_count+1}/{max_retries}): {str(e)}")
            retry_count += 1
            time.sleep(2)  # Add delay before retry
        
        except Exception as e:
            logging.error(f"Unexpected error in ComfyUI workflow (attempt {retry_count+1}/{max_retries}): {str(e)}")
            retry_count += 1
            time.sleep(2)  # Add delay before retry
    
    logging.error("All ComfyUI workflow attempts failed")
    
    # Create fallback image
    try:
        Image.new('RGB', (512, 512), color=(0, 0, 128)).save(file_path)
        logging.info(f"Created fallback blue square image at {file_path}")
        return file_path
    except Exception as e:
        logging.error(f"Failed to create fallback image: {str(e)}")
        return None


def generate_veo_video(prompt, image_path, save_path, config, max_retries=3):
    """
    Generates a video clip using Google's VEO 2 API with fallback options.
    
    Args:
        prompt (str): Video generation prompt
        image_path (str): Path to the input image to use in the video
        save_path (str): Directory to save the video
        config (dict): Application configuration
        max_retries (int): Maximum number of retry attempts
        
    Returns:
        str: Path to generated video or None if failed
    """
    logging.info(f"Generating VEO video for prompt: {prompt[:100]}...")
    
    if not image_path or not os.path.exists(image_path):
        logging.error(f"Input image not found: {image_path}")
        return None
    
    # Create timestamp for unique filename
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    file_path = os.path.join(save_path, f"veo_{timestamp}.mp4")
    
    # Check if Google API key is available
    if not config["google_api_key"]:
        logging.error("Google API key not available for VEO 2")
        return None
    
    retry_count = 0
    while retry_count < max_retries:
        try:
            # Note: VEO 2 is a hypothetical API for video generation based on Gemini
            # This is a simplified implementation based on available documentation
            # The actual implementation would require the specific VEO API details
            
            # 1. Read the image
            with open(image_path, "rb") as f:
                image_bytes = f.read()
                image_b64 = base64.b64encode(image_bytes).decode("utf-8")
            
            # 2. Prepare API request (simplified example)
            model = genai.GenerativeModel('gemini-pro-vision')
            
            # Prepare combined prompt with instructions
            video_prompt = f"""
            Generate a surreal, artistic video animation based on this image.
            Style: Raw, gritty, mind-bending
            Duration: 8 seconds
            Specific instructions: {prompt}
            Make the video dynamic with camera movement and visual transformations.
            """
            
            # Make API call with image and text
            content = [
                {
                    "text": video_prompt,
                    "image": {"inline_data": {"data": image_b64, "mime_type": "image/png"}}
                }
            ]
            
            # This is a stub for an actual VEO API call
            # In a real implementation, would use the actual VEO endpoints
            # response = model.generate_video(content)
            
            # For now, simulate API failure to trigger fallback
            raise NotImplementedError("VEO 2 API integration not yet implemented")
        
        except Exception as e:
            logging.error(f"Error in VEO 2 video generation (attempt {retry_count+1}/{max_retries}): {str(e)}")
            retry_count += 1
            
            # If VEO fails, try Kling API as fallback
            if config["kling_api_key"]:
                logging.info("Falling back to Kling AI for video generation")
                kling_result = generate_kling_video(prompt, image_path, save_path, config)
                if kling_result:
                    return kling_result
            
            # If both fail, create a basic animated video from the image
            logging.warning("Creating fallback animation from still image")
            try:
                from moviepy.editor import ImageClip, vfx
                
                # Create a video clip from the image
                img_clip = ImageClip(image_path, duration=8)
                
                # Apply some effects to make it animated
                # Zoom in slowly
                animated_clip = img_clip.fx(vfx.resize, lambda t: 1 + 0.1*t)
                # Add subtle position movement
                animated_clip = animated_clip.fx(
                    vfx.move_fn,
                    lambda t: ('center', 'center') if t < 0.5 else ('center', 540 - 10*t)
                )
                
                # Write to file
                animated_clip.write_videofile(file_path, fps=24, codec='libx264', audio=False, logger=None)
                
                # Check if file was created successfully
                if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
                    logging.info(f"Fallback video animation created at {file_path}")
                    return file_path
                else:
                    logging.warning(f"Fallback video file is empty or missing: {file_path}")
            
            except Exception as e:
                logging.error(f"Error creating fallback video: {str(e)}")
    
    logging.error("All video generation attempts failed")
    return None


def generate_kling_video(prompt, image_path, save_path, config, max_retries=3):
    """
    Generates a video clip using Kling AI API with fallback to local animation.
    
    Args:
        prompt (str): Video generation prompt
        image_path (str): Path to the input image to use in the video
        save_path (str): Directory to save the video
        config (dict): Application configuration
        max_retries (int): Maximum number of retry attempts
        
    Returns:
        str: Path to generated video or None if failed
    """
    logging.info(f"Generating Kling video for prompt: {prompt[:100]}...")
    
    if not image_path or not os.path.exists(image_path):
        logging.error(f"Input image not found: {image_path}")
        return None
    
    # Create timestamp for unique filename
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    file_path = os.path.join(save_path, f"kling_{timestamp}.mp4")
    
    # Check if Kling API key and endpoint are available
    if not config["kling_api_key"] or not config["kling_api_endpoint"]:
        logging.error("Kling API key or endpoint not available")
        return generate_fallback_video(image_path, file_path)
    
    retry_count = 0
    while retry_count < max_retries:
        try:
            # Prepare API request to Kling AI
            with open(image_path, "rb") as f:
                files = {"image": (os.path.basename(image_path), f, "image/png")}
                
                payload = {
                    "prompt": prompt,
                    "duration": 8,  # 8 seconds
                    "style": "surreal",
                    "motion": "medium"
                }
                
                headers = {
                    "Authorization": f"Bearer {config['kling_api_key']}"
                }
                
                # Make API request
                response = requests.post(
                    config["kling_api_endpoint"],
                    files=files,
                    data=payload,
                    headers=headers,
                    timeout=60  # Allow up to 60 seconds for video generation
                )
                
                response.raise_for_status()
                
                # Parse response
                result = response.json()
                
                if "video_url" in result:
                    # Download the video
                    video_url = result["video_url"]
                    video_response = requests.get(video_url, stream=True)
                    video_response.raise_for_status()
                    
                    with open(file_path, 'wb') as f:
                        for chunk in video_response.iter_content(8192):
                            f.write(chunk)
                    
                    # Verify file exists and has content
                    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
                        logging.info(f"Kling video generated and saved to {file_path}")
                        return file_path
                    else:
                        logging.warning(f"Kling video file is empty or missing: {file_path}")
                        retry_count += 1
                else:
                    logging.warning(f"Kling API did not return a video URL: {result}")
                    retry_count += 1
        
        except requests.exceptions.RequestException as e:
            logging.error(f"Kling API request error (attempt {retry_count+1}/{max_retries}): {str(e)}")
            retry_count += 1
            time.sleep(2)  # Add delay before retry
        
        except Exception as e:
            logging.error(f"Unexpected error in Kling video generation (attempt {retry_count+1}/{max_retries}): {str(e)}")
            retry_count += 1
            time.sleep(2)  # Add delay before retry
    
    # All attempts failed, create fallback video
    logging.error("All Kling video generation attempts failed")
    return generate_fallback_video(image_path, file_path)


def generate_fallback_video(image_path, output_path):
    """
    Creates a fallback video animation from a still image using MoviePy.
    Used when all external API video generation attempts fail.
    
    Args:
        image_path (str): Path to the input image
        output_path (str): Path for the output video
        
    Returns:
        str: Path to generated video or None if failed
    """
    logging.info("Generating fallback video animation from image...")
    
    try:
        from moviepy.editor import ImageClip, CompositeVideoClip, vfx
        
        # Load the image and create a base clip
        img_clip = ImageClip(image_path, duration=8)
        
        # Create multiple effects for a more interesting video
        clips = []
        
        # Base clip with zoom effect
        zoom_clip = img_clip.fx(vfx.resize, lambda t: max(1, 1 + 0.05*t))
        clips.append(zoom_clip)
        
        # Create a composition
        final_clip = CompositeVideoClip(clips)
        
        # Write to file with high quality
        final_clip.write_videofile(
            output_path, 
            fps=24, 
            codec='libx264',
            audio=False,
            bitrate="5000k",
            logger=None
        )
        
        if os.path.exists(output_path) and os.path.getsize(output_path) > 0:
            logging.info(f"Fallback video animation created at {output_path}")
            return output_path
        else:
            logging.error(f"Failed to create fallback video: file missing or empty")
            return None
    
    except Exception as e:
        logging.error(f"Error creating fallback video: {str(e)}")
        
        # Last resort: create an absolute minimal video clip
        try:
            from moviepy.editor import ColorClip
            
            # Create a simple color clip as absolute fallback
            clip = ColorClip(size=(640, 640), color=(0, 0, 255), duration=5)
            clip.write_videofile(output_path, fps=24, logger=None)
            
            logging.info(f"Minimal fallback video created at {output_path}")
            return output_path
        except Exception as e2:
            logging.error(f"Error creating minimal fallback video: {str(e2)}")
            return None


# --- Video Assembly Functions ---

def apply_trend_style(clip, style_name: str, intensity: float = 0.5):
    """
    Applies a trendy visual effect to a video clip based on the style name.
    
    Args:
        clip: MoviePy video clip
        style_name (str): Name of the visual style to apply
        intensity (float): Intensity of the effect (0.0-1.0)
        
    Returns:
        MoviePy video clip with effect applied
    """
    try:
        # Limit intensity to valid range
        intensity = max(0.1, min(1.0, intensity))
        
        if style_name == "glitchcore":
            # Apply glitch effects
            # Randomly offset RGB channels
            def glitch_effect(get_frame, t):
                frame = get_frame(t)
                if random.random() < intensity * 0.2:  # Random glitching
                    # Random channel offset
                    offset_x = int(random.randint(-20, 20) * intensity)
                    offset_y = int(random.randint(-20, 20) * intensity)
                    
                    # Get the RGB channels
                    r = frame[:, :, 0]
                    g = frame[:, :, 1]
                    b = frame[:, :, 2]
                    
                    # Create a new frame with offset channels
                    height, width = frame.shape[:2]
                    new_frame = np.zeros_like(frame)
                    
                    # Apply offsets differently to each channel
                    if offset_x > 0:
                        new_frame[:, offset_x:, 0] = r[:, :(width-offset_x)]
                    elif offset_x < 0:
                        new_frame[:, :offset_x, 0] = r[:, -offset_x:]
                    else:
                        new_frame[:, :, 0] = r
                        
                    if offset_y > 0:
                        new_frame[offset_y:, :, 1] = g[:(height-offset_y), :]
                    elif offset_y < 0:
                        new_frame[:offset_y, :, 1] = g[-offset_y:, :]
                    else:
                        new_frame[:, :, 1] = g
                        
                    new_frame[:, :, 2] = b  # Keep blue channel unchanged
                    
                    return new_frame
                return frame
            
            return clip.fl(glitch_effect)
            
        elif style_name == "vaporwave":
            # Purple/pink/blue color grading + slight VHS effect
            def vaporwave_effect(get_frame, t):
                frame = get_frame(t)
                # Enhance blue and red, reduce green
                frame = frame.astype(float)
                frame[:, :, 0] = np.clip(frame[:, :, 0] * 1.1, 0, 255)  # Boost red
                frame[:, :, 1] = np.clip(frame[:, :, 1] * 0.85, 0, 255)  # Reduce green
                frame[:, :, 2] = np.clip(frame[:, :, 2] * 1.2, 0, 255)  # Boost blue
                return frame.astype('uint8')
            
            # Apply color effect and slight blur
            processed = clip.fl(vaporwave_effect)
            if intensity > 0.7:
                processed = processed.fx(vfx.colorx, 1.2)  # Increase saturation
            return processed
            
        elif style_name == "weirdcore":
            # Nostalgic, dreamlike, uncanny effects
            effects = []
            
            # 1. Add slight VHS tracking effect occasionally
            if random.random() < intensity * 0.5:
                def tracking_lines(get_frame, t):
                    frame = get_frame(t)
                    if random.random() < intensity * 0.1:  # Occasional lines
                        line_pos = random.randint(0, frame.shape[0] - 1)
                        line_height = random.randint(1, 5)
                        frame[line_pos:line_pos+line_height, :, :] = 255
                    return frame
                effects.append(tracking_lines)
            
            # 2. Add slight color shift
            def weirdcore_colors(get_frame, t):
                frame = get_frame(t)
                # Subtle color alterations to make things uncanny
                frame = frame.astype(float)
                frame[:, :, random.randint(0, 2)] = np.clip(
                    frame[:, :, random.randint(0, 2)] * (0.9 + random.random() * 0.2),
                    0, 255
                )
                return frame.astype('uint8')
            effects.append(weirdcore_colors)
            
            # Apply all selected effects
            processed = clip
            for effect in effects:
                processed = processed.fl(effect)
            
            # Add slight blur for dreamlike quality
            if intensity > 0.4:
                processed = processed.fx(vfx.gaussian_blur, sigma=0.5)
            
            return processed
            
        elif style_name == "cyberpunk":
            # High contrast neon effect
            def cyberpunk_effect(get_frame, t):
                frame = get_frame(t)
                # Enhance contrast
                frame = frame.astype(float)
                # Boost neon colors (especially cyan and magenta)
                frame[:, :, 0] = np.clip(frame[:, :, 0] * 1.2, 0, 255)  # Red
                frame[:, :, 2] = np.clip(frame[:, :, 2] * 1.2, 0, 255)  # Blue
                return frame.astype('uint8')
            
            processed = clip.fl(cyberpunk_effect)
            # Increase contrast
            processed = processed.fx(vfx.colorx, 1.2)
            return processed
            
        elif style_name == "y2k":
            # Y2K aesthetic (bubblegum, futuristic early 2000s digital)
            def y2k_effect(get_frame, t):
                frame = get_frame(t)
                # High saturation, slightly increased brightness
                frame = frame.astype(float)
                # Boost vibrance
                frame = np.clip(frame * 1.1, 0, 255)
                return frame.astype('uint8')
            
            return clip.fl(y2k_effect).fx(vfx.colorx, 1.15)
            
        else:
            # If unknown style, apply a moderate saturation boost as default
            return clip.fx(vfx.colorx, 1.1)
    
    except Exception as e:
        logging.error(f"Error applying {style_name} effect: {str(e)}")
        # Return original clip if effect application fails
        return clip


def apply_viral_enhancements(clip, intensity: float = 0.7):
    """
    Applies viral-friendly video enhancements including fast cuts, zooms, and effects.
    
    Args:
        clip: MoviePy video clip
        intensity (float): Intensity of enhancements (0.0-1.0)
        
    Returns:
        MoviePy video clip with enhancements
    """
    try:
        # Only apply enhancements if intensity is significant
        if intensity < 0.2:
            return clip
            
        clip_duration = clip.duration
        
        # 1. Decide which enhancements to apply based on intensity
        enhancements = []
        
        # Fast zoom in/out (more likely with higher intensity)
        if random.random() < intensity * 0.7:
            zoom_factor = 1.0 + (intensity * 0.5)  # Max 50% zoom
            zoom_duration = clip_duration * 0.7  # Use 70% of the clip for zoom
            
            # Randomly choose zoom in or zoom out
            if random.random() > 0.5:
                # Zoom in
                zoom_effect = clip.fx(
                    vfx.resize, 
                    lambda t: 1 + (zoom_factor - 1) * min(1.0, t / zoom_duration)
                )
            else:
                # Zoom out
                zoom_effect = clip.fx(
                    vfx.resize, 
                    lambda t: zoom_factor - (zoom_factor - 1) * min(1.0, t / zoom_duration)
                )
            
            enhancements.append(zoom_effect)
        else:
            enhancements.append(clip)
        
        # 2. Add intermittent speed changes if intensity is high
        if intensity > 0.6 and clip_duration > 3.0:
            # Get the current enhanced clip
            current_clip = enhancements[-1]
            
            # Create segments with different speeds
            num_segments = random.randint(2, 3)
            segments = []
            
            segment_duration = clip_duration / num_segments
            for i in range(num_segments):
                start_time = i * segment_duration
                end_time = min((i + 1) * segment_duration, clip_duration)
                
                segment = current_clip.subclip(start_time, end_time)
                
                # Apply random speed factor
                if i % 2 == 1:  # Every other segment
                    speed_factor = random.uniform(0.8, 1.5)
                    segment = segment.fx(vfx.speedx, speed_factor)
                
                segments.append(segment)
            
            # Replace the last enhancement with the speed-modified version
            speed_modified = concatenate_videoclips(segments)
            enhancements[-1] = speed_modified
        
        # 3. Add subtle shake effect if intensity is very high
        if intensity > 0.8:
            current_clip = enhancements[-1]
            
            def shake_effect(get_frame, t):
                # Only apply shake occasionally
                if random.random() < 0.3:
                    max_offset = int(5 * intensity)
                    offset_x = random.randint(-max_offset, max_offset)
                    offset_y = random.randint(-max_offset, max_offset)
                    
                    frame = get_frame(t)
                    height, width = frame.shape[:2]
                    
                    # Create canvas
                    result = np.zeros_like(frame)
                    
                    # Calculate source and destination regions
                    src_x = max(0, offset_x) if offset_x < 0 else 0
                    src_y = max(0, offset_y) if offset_y < 0 else 0
                    src_w = width - abs(offset_x)
                    src_h = height - abs(offset_y)
                    
                    dst_x = 0 if offset_x < 0 else offset_x
                    dst_y = 0 if offset_y < 0 else offset_y
                    
                    # Copy shifted region
                    result[dst_y:dst_y+src_h, dst_x:dst_x+src_w] = frame[src_y:src_y+src_h, src_x:src_x+src_w]
                    return result
                else:
                    return get_frame(t)
            
            shake_clip = current_clip.fl(shake_effect)
            enhancements[-1] = shake_clip
        
        # Return the final enhanced clip
        return enhancements[-1]
    
    except Exception as e:
        logging.error(f"Error applying viral enhancements: {str(e)}")
        # Return original clip if enhancement fails
        return clip


def create_text_overlay(text: str, clip_size: Tuple[int, int], duration: float, style: str = "default"):
    """
    Creates a stylized text overlay for the video.
    
    Args:
        text (str): Text to display
        clip_size (tuple): Width and height of the video
        duration (float): Duration for the text display
        style (str): Text style to apply
        
    Returns:
        MoviePy TextClip object
    """
    try:
        width, height = clip_size
        fontsize = int(height * 0.05)  # Scale text size based on video height
        
        # Split text into lines for better visibility (max 25 chars per line)
        words = text.split()
        lines = []
        current_line = []
        
        for word in words:
            if current_line and len(' '.join(current_line + [word])) > 25:
                lines.append(' '.join(current_line))
                current_line = [word]
            else:
                current_line.append(word)
        
        if current_line:
            lines.append(' '.join(current_line))
        
        formatted_text = '\n'.join(lines)
        
        # Style-based formatting
        if style == "glitchcore":
            txt_clip = TextClip(
                formatted_text, 
                fontsize=fontsize,
                color='white',
                font='Arial-Bold',
                stroke_color='black',
                stroke_width=2,
                align='center',
                method='caption'
            )
            
            # Add glitch animation effect
            def glitch_text(t):
                # Occasionally shift position
                if random.random() < 0.2:
                    offset_x = random.randint(-10, 10)
                    offset_y = random.randint(-10, 10)
                    return ('center', 'center', offset_x, offset_y)
                return ('center', 'center')
            
            txt_clip = txt_clip.set_position(glitch_text)
            
        elif style == "vaporwave":
            txt_clip = TextClip(
                formatted_text, 
                fontsize=fontsize,
                color='#FF00FF',  # Hot pink
                font='Arial-Bold',
                stroke_color='#00FFFF',  # Cyan
                stroke_width=2,
                align='center',
                method='caption'
            )
            txt_clip = txt_clip.set_position('center')
            
        elif style == "minimalist":
            txt_clip = TextClip(
                formatted_text, 
                fontsize=fontsize,
                color='white',
                font='Arial',
                bg_color='rgba(0,0,0,0.5)',
                align='center',
                method='caption'
            )
            txt_clip = txt_clip.set_position('center')
            
        else:  # Default style
            txt_clip = TextClip(
                formatted_text, 
                fontsize=fontsize,
                color='white',
                font='Arial-Bold',
                stroke_color='black',
                stroke_width=1,
                align='center',
                method='caption'
            )
            txt_clip = txt_clip.set_position('center')
        
        # Set duration and fading
        txt_clip = txt_clip.set_duration(duration)
        
        # Add fade in/out
        fade_duration = min(0.5, duration / 4)
        txt_clip = txt_clip.fadein(fade_duration).fadeout(fade_duration)
        
        return txt_clip
    
    except Exception as e:
        logging.error(f"Error creating text overlay: {str(e)}")
        # Create a simple fallback text clip
        try:
            simple_txt = TextClip(
                text, 
                fontsize=height//20,
                color='white',
                bg_color='rgba(0,0,0,0.5)',
                size=(width * 0.8, None),
                method='caption'
            ).set_position('center').set_duration(duration)
            
            return simple_txt
        except Exception as e2:
            logging.error(f"Failed to create fallback text overlay: {str(e2)}")
            return None


def assemble_video(
    image_paths: List[str], 
    video_clip_paths: List[str], 
    audio_path: str, 
    text_overlay: str, 
    music_path: str, 
    output_path: str,
    visual_style: str = "default",
    viral_intensity: float = 0.7
):
    """
    Assembles a final video from various assets, applying effects and combining elements.
    
    Args:
        image_paths (list): Paths to input images
        video_clip_paths (list): Paths to input video clips
        audio_path (str): Path to voiceover audio
        text_overlay (str): Text to display in the video
        music_path (str): Path to background music/sound
        output_path (str): Path for the output video
        visual_style (str): Visual style to apply
        viral_intensity (float): Intensity of viral enhancements
        
    Returns:
        str: Path to the assembled video or None if assembly failed
    """
    logging.info(f"Assembling video from {len(video_clip_paths)} clips and {len(image_paths)} images")
    
    try:
        clips = []
        
        # 1. Load generated video clips if available
        for path in video_clip_paths:
            if path and os.path.exists(path):
                try:
                    clip = VideoFileClip(path)
                    if clip.duration > 0:
                        clips.append(clip)
                        logging.info(f"Loaded video clip: {path} (duration: {clip.duration:.2f}s)")
                    else:
                        logging.warning(f"Skipping zero-duration video clip: {path}")
                except Exception as e:
                    logging.error(f"Failed to load video clip {path}: {str(e)}")
        
        # 2. If no valid video clips, create clips from images
        if not clips and image_paths:
            for img_path in image_paths:
                if img_path and os.path.exists(img_path):
                    try:
                        # Make each image clip 3-5 seconds
                        img_duration = random.uniform(3.0, 5.0)
                        img_clip = ImageClip(img_path).set_duration(img_duration)
                        
                        # Apply random animation to make it more dynamic
                        animation_choice = random.choice(["zoom", "pan", "none"])
                        
                        if animation_choice == "zoom":
                            zoom_factor = random.uniform(1.0, 1.2)
                            img_clip = img_clip.fx(
                                vfx.resize, 
                                lambda t: 1 + (zoom_factor - 1) * t / img_duration
                            )
                        elif animation_choice == "pan":
                            # Create subtle panning effect
                            def pan_effect(t):
                                progress = t / img_duration
                                x_shift = 0.1 * np.sin(2 * np.pi * progress)
                                y_shift = 0.05 * np.cos(2 * np.pi * progress)
                                return ('center', 'center', x_shift, y_shift)
                            
                            img_clip = img_clip.set_position(pan_effect)
                        
                        clips.append(img_clip)
                        logging.info(f"Created animated clip from image: {img_path} (duration: {img_duration:.2f}s)")
                    except Exception as e:
                        logging.error(f"Failed to create clip from image {img_path}: {str(e)}")
        
        # 3. If still no clips, create a colorful placeholder clip
        if not clips:
            logging.warning("No valid video clips or images. Creating placeholder clip.")
            
            # Create a gradient background
            w, h = 720, 1280  # Portrait format for social media
            gradient_clip = ColorClip(size=(w, h), color=(0, 0, 0), duration=10)
            
            # Add animated gradient using a custom numpy array
            def make_gradient_frame(t):
                frame = np.zeros((h, w, 3), dtype=np.uint8)
                for i in range(h):
                    r = int(255 * (0.5 + 0.5 * np.sin(i / 50 + t * 2)))
                    g = int(255 * (0.5 + 0.5 * np.sin(i / 50 + t * 2 + 2)))
                    b = int(255 * (0.5 + 0.5 * np.sin(i / 50 + t * 2 + 4)))
                    frame[i, :, 0] = r
                    frame[i, :, 1] = g
                    frame[i, :, 2] = b
                return frame
            
            gradient_frames = [make_gradient_frame(t/24) for t in range(240)]  # 10 seconds at 24fps
            gradient_clip = ImageSequenceClip(gradient_frames, fps=24)
            
            clips.append(gradient_clip)
            logging.info("Created placeholder gradient clip")
        
        # 4. Concatenate or composite clips
        if len(clips) > 1:
            # Add transitions between clips
            clips_with_transitions = []
            transition_duration = 0.5  # Half a second transition
            
            for i, clip in enumerate(clips):
                # Apply fade in/out for smooth transitions
                if i > 0:  # Not the first clip
                    clip = clip.fadein(transition_duration)
                if i < len(clips) - 1:  # Not the last clip
                    clip = clip.fadeout(transition_duration)
                
                clips_with_transitions.append(clip)
            
            # Concatenate with crossfadeout
            final_clip = concatenate_videoclips(
                clips_with_transitions, 
                method="crossfadeout",
                crossfade_duration=transition_duration
            )
            
            logging.info(f"Concatenated {len(clips)} clips with transitions")
        else:
            # Only one clip, use it directly
            final_clip = clips[0]
        
        # 5. Apply visual style effects based on the style parameter
        final_clip = apply_trend_style(final_clip, visual_style)
        logging.info(f"Applied {visual_style} visual style")
        
        # 6. Apply viral enhancements (zoom, speed changes, etc.)
        final_clip = apply_viral_enhancements(final_clip, viral_intensity)
        logging.info(f"Applied viral enhancements with intensity {viral_intensity}")
        
        # 7. Add text overlay if provided
        if text_overlay:
            txt_clip = create_text_overlay(
                text_overlay,
                (final_clip.w, final_clip.h),
                final_clip.duration,
                style=visual_style
            )
            
            if txt_clip:
                final_clip = CompositeVideoClip([final_clip, txt_clip])
                logging.info("Added text overlay")
        
        # 8. Handle audio (voiceover + background music)
        audio_tracks = []
        
        # Add voiceover if available
        if audio_path and os.path.exists(audio_path):
            try:
                voiceover = AudioFileClip(audio_path)
                
                # Ensure audio doesn't exceed video duration
                if voiceover.duration > final_clip.duration:
                    voiceover = voiceover.subclip(0, final_clip.duration)
                
                audio_tracks.append(voiceover)
                logging.info(f"Added voiceover audio: {audio_path}")
            except Exception as e:
                logging.error(f"Failed to add voiceover {audio_path}: {str(e)}")
        
        # Add background music if available
        if music_path and os.path.exists(music_path):
            try:
                background_music = AudioFileClip(music_path)
                
                # Loop or trim music to match video duration
                if background_music.duration < final_clip.duration:
                    # Calculate how many loops we need
                    n_loops = int(np.ceil(final_clip.duration / background_music.duration))
                    background_music = concatenate_videoclips([background_music] * n_loops)
                
                # Trim to video duration
                background_music = background_music.subclip(0, final_clip.duration)
                
                # Lower volume for background
                background_music = background_music.volumex(0.3)
                
                audio_tracks.append(background_music)
                logging.info(f"Added background music: {music_path}")
            except Exception as e:
                logging.error(f"Failed to add background music {music_path}: {str(e)}")
        
        # Combine audio tracks if we have multiple
        if len(audio_tracks) > 1:
            final_audio = CompositeAudioClip(audio_tracks)
            final_clip = final_clip.set_audio(final_audio)
            logging.info("Combined multiple audio tracks")
        elif len(audio_tracks) == 1:
            final_clip = final_clip.set_audio(audio_tracks[0])
        
        # 9. Final adjustments
        # Ensure clip isn't too long for social media (max 60 seconds)
        if final_clip.duration > 60:
            final_clip = final_clip.subclip(0, 60)
            logging.info("Trimmed video to 60 seconds maximum")
        
        # 10. Write final video file with high quality settings
        logging.info(f"Writing final video to {output_path}...")
        
        final_clip.write_videofile(
            output_path,
            codec='libx264',
            audio_codec='aac',
            bitrate='5000k',
            fps=24,
            threads=2,
            logger=None  # Disable MoviePy progress bars in logs
        )
        
        # 11. Clean up and close clips to release resources
        for clip in clips:
            clip.close()
        
        if len(audio_tracks) > 0:
            for audio in audio_tracks:
                audio.close()
        
        final_clip.close()
        
        # Verify the output file exists and has content
        if os.path.exists(output_path) and os.path.getsize(output_path) > 0:
            logging.info(f"Video assembly complete: {output_path}")
            return output_path
        else:
            logging.error(f"Output video file missing or empty: {output_path}")
            return None
    
    except Exception as e:
        logging.error(f"Error in video assembly: {str(e)}")
        return None


# --- Social Media Integration ---

def generate_metadata(content_theme: str, trending_hashtags: List[str], config: Dict, max_retries: int = 3) -> Dict:
    """
    Generates platform-specific metadata (titles, descriptions, tags) using an LLM.
    
    Args:
        content_theme (str): The content theme to base metadata on
        trending_hashtags (list): List of trending hashtags to incorporate
        config (dict): Application configuration
        max_retries (int): Maximum number of retry attempts
        
    Returns:
        dict: Dictionary with metadata for different platforms
    """
    logging.info(f"Generating metadata for theme: {content_theme}")
    
    # Prepare trending hashtags string
    hashtag_string = " ".join(trending_hashtags[:5])  # Use top 5 hashtags
    
    # Create prompt for metadata generation
    prompt = f"""
    Generate engaging metadata for a surreal, mind-bending short video with the theme: "{content_theme}"
    
    The video contains an AI-generated surreal affirmation that makes viewers think.
    
    Please provide the following in the exact format shown:
    
    YOUTUBE_TITLE: [Create an attention-grabbing title under 100 characters]
    YOUTUBE_DESCRIPTION: [Write 2-3 sentences that describe the video in an intriguing way plus a call to action]
    YOUTUBE_TAGS: [10-15 relevant tags separated by commas]
    
    TIKTOK_CAPTION: [Write a catchy, mysterious caption under 150 characters]
    TIKTOK_HASHTAGS: [Create 5-8 hashtags including these trending ones: {hashtag_string}]
    """
    
    # Use the LLM to generate metadata with retry logic
    for attempt in range(max_retries):
        try:
            llm_response = generate_llm_text(prompt, config)
            
            if not llm_response:
                raise ValueError("Empty response from LLM")
            
            # Parse the response
            metadata = {}
            current_key = None
            
            for line in llm_response.split('\n'):
                line = line.strip()
                if not line:
                    continue
                
                # Check for metadata keys
                if line.startswith('YOUTUBE_TITLE:'):
                    current_key = 'youtube_title'
                    metadata[current_key] = line.replace('YOUTUBE_TITLE:', '').strip()
                elif line.startswith('YOUTUBE_DESCRIPTION:'):
                    current_key = 'youtube_description'
                    metadata[current_key] = line.replace('YOUTUBE_DESCRIPTION:', '').strip()
                elif line.startswith('YOUTUBE_TAGS:'):
                    current_key = 'youtube_tags'
                    tags_text = line.replace('YOUTUBE_TAGS:', '').strip()
                    metadata[current_key] = [tag.strip() for tag in tags_text.split(',')]
                elif line.startswith('TIKTOK_CAPTION:'):
                    current_key = 'tiktok_caption'
                    metadata[current_key] = line.replace('TIKTOK_CAPTION:', '').strip()
                elif line.startswith('TIKTOK_HASHTAGS:'):
                    current_key = 'tiktok_hashtags'
                    metadata[current_key] = line.replace('TIKTOK_HASHTAGS:', '').strip()
                elif current_key:
                    # Append to current metadata entry for multi-line values
                    metadata[current_key] += " " + line
            
            # Validate all required fields are present
            required_fields = ['youtube_title', 'youtube_description', 'youtube_tags', 
                              'tiktok_caption', 'tiktok_hashtags']
            
            missing_fields = [field for field in required_fields if field not in metadata]
            
            if missing_fields:
                logging.warning(f"Missing metadata fields: {missing_fields}. Retrying...")
                continue
            
            # Ensure YouTube title is within limits
            if len(metadata['youtube_title']) > 100:
                metadata['youtube_title'] = metadata['youtube_title'][:97] + '...'
                
            # Ensure TikTok caption is within limits
            if len(metadata['tiktok_caption']) > 150:
                metadata['tiktok_caption'] = metadata['tiktok_caption'][:147] + '...'
            
            logging.info(f"Generated metadata successfully: {metadata['youtube_title']}")
            return metadata
            
        except Exception as e:
            logging.error(f"Error generating metadata (attempt {attempt+1}/{max_retries}): {str(e)}")
            time.sleep(2)  # Add delay before retry
    
    # If all attempts fail, generate a simple fallback
    logging.error("All metadata generation attempts failed. Using fallback metadata.")
    
    # Create simple metadata with the theme and current date
    today = datetime.now().strftime('%Y-%m-%d')
    return {
        'youtube_title': f"Surreal Affirmation: {content_theme}",
        'youtube_description': f"A mind-bending AI-generated affirmation about {content_theme}. Generated on {today}.",
        'youtube_tags': ['AI', 'Surreal', 'Affirmation', 'AI Art', 'Surrealism', 'Digital Art'],
        'tiktok_caption': f"When reality glitches... #AI #Surreal #{content_theme.replace(' ', '')}",
        'tiktok_hashtags': hashtag_string
    }


def get_youtube_service(config: Dict) -> Optional[object]:
    """
    Authenticates and builds the YouTube API service object using OAuth 2.0.
    Handles token refresh and first-time authentication.
    
    Args:
        config (dict): Application configuration
        
    Returns:
        object: YouTube API service object or None if authentication failed
    """
    logging.info("Setting up YouTube API service")
    
    # Define constants
    CLIENT_SECRETS_FILE = "client_secret.json"
    SCOPES = ['https://www.googleapis.com/auth/youtube.upload']
    API_SERVICE_NAME = 'youtube'
    API_VERSION = 'v3'
    CREDENTIALS_FILE = config.get("google_api_credentials_file", "youtube_credentials.json")
    
    creds = None
    
    # Check if we have valid credentials
    if os.path.exists(CREDENTIALS_FILE):
        try:
            creds = Credentials.from_authorized_user_file(CREDENTIALS_FILE, SCOPES)
            logging.info("Loaded existing YouTube credentials")
        except Exception as e:
            logging.error(f"Error loading credentials file: {str(e)}")
            creds = None
    
    # If credentials missing or invalid, attempt to refresh or create new ones
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            try:
                logging.info("Refreshing expired YouTube credentials")
                creds.refresh(Request())
                
                # Save refreshed credentials
                with open(CREDENTIALS_FILE, 'w') as token:
                    token.write(creds.to_json())
                    
                logging.info("YouTube credentials refreshed successfully")
            except Exception as e:
                logging.error(f"Error refreshing credentials: {str(e)}")
                creds = None
        
        # If refresh failed or we have no credentials, need to authenticate
        if not creds:
            # Check if client secrets file exists
            if not os.path.exists(CLIENT_SECRETS_FILE):
                logging.error(f"OAuth secrets file '{CLIENT_SECRETS_FILE}' not found")
                logging.error("Please download the client_secret.json file from Google Cloud Console")
                logging.error("See: https://console.cloud.google.com/ > APIs & Services > Credentials")
                return None
            
            try:
                logging.info("Starting new YouTube authentication flow")
                flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRETS_FILE, SCOPES)
                
                # Note: This requires user interaction in a browser
                creds = flow.run_local_server(port=0)
                
                # Save the credentials for future use
                with open(CREDENTIALS_FILE, 'w') as token:
                    token.write(creds.to_json())
                    
                logging.info(f"New YouTube credentials saved to {CREDENTIALS_FILE}")
            except Exception as e:
                logging.error(f"Error in authentication flow: {str(e)}")
                return None
    
    # Build the YouTube service with our credentials
    try:
        service = build(API_SERVICE_NAME, API_VERSION, credentials=creds)
        logging.info("YouTube service built successfully")
        return service
    except Exception as e:
        logging.error(f"Error building YouTube service: {str(e)}")
        return None


def upload_to_youtube(
    video_path: str, 
    title: str, 
    description: str, 
    tags: List[str], 
    config: Dict,
    privacy_status: str = "public",
    category_id: str = "22",  # 22 = People & Blogs
    max_retries: int = 3
) -> bool:
    """
    Uploads a video to YouTube with metadata and handles potential errors.
    
    Args:
        video_path (str): Path to the video file to upload
        title (str): Video title
        description (str): Video description
        tags (list): List of tags
        config (dict): Application configuration
        privacy_status (str): Privacy setting ("public", "private", "unlisted")
        category_id (str): YouTube category ID
        max_retries (int): Maximum number of retry attempts
        
    Returns:
        bool: True if upload was successful, False otherwise
    """
    logging.info(f"Uploading video to YouTube: {title}")
    
    # Validate inputs
    if not video_path or not os.path.exists(video_path):
        logging.error(f"Video file not found: {video_path}")
        return False
    
    if not title or not description:
        logging.error("Missing required metadata (title or description)")
        return False
    
    # Get authenticated YouTube service
    youtube = get_youtube_service(config)
    if not youtube:
        logging.error("Failed to get YouTube service. Aborting upload.")
        return False
    
    # Attempt upload with retry logic
    for attempt in range(max_retries):
        try:
            # Prepare video metadata
            body = {
                'snippet': {
                    'title': title,
                    'description': description,
                    'tags': tags,
                    'categoryId': category_id,
                    'defaultLanguage': 'en',
                    'defaultAudioLanguage': 'en'
                },
                'status': {
                    'privacyStatus': privacy_status,
                    'selfDeclaredMadeForKids': False,
                    'publishAt': None  # Publish immediately
                }
            }
            
            # Prepare the video file for upload
            media = MediaFileUpload(
                video_path, 
                mimetype='video/mp4',
                chunksize=1024*1024,
                resumable=True
            )
            
            # Create upload request
            request = youtube.videos().insert(
                part=','.join(body.keys()),
                body=body,
                media_body=media,
                notifySubscribers=True
            )
            
            # Execute the upload with progress tracking
            logging.info("Starting YouTube upload...")
            response = None
            
            while response is None:
                status, response = request.next_chunk()
                if status:
                    progress = int(status.progress() * 100)
                    logging.info(f"YouTube upload progress: {progress}%")
            
            video_id = response.get('id')
            if not video_id:
                logging.error("YouTube upload completed but no video ID returned")
                return False
            
            video_url = f"https://www.youtube.com/watch?v={video_id}"
            logging.info(f"YouTube upload successful! Video ID: {video_id}")
            logging.info(f"Video URL: {video_url}")
            
            # Optional: Add the video to a playlist
            # playlist_id = config.get('youtube_playlist_id')
            # if playlist_id:
            #     try:
            #         youtube.playlistItems().insert(
            #             part="snippet",
            #             body={
            #                 "snippet": {
            #                     "playlistId": playlist_id,
            #                     "resourceId": {
            #                         "kind": "youtube#video",
            #                         "videoId": video_id
            #                     }
            #                 }
            #             }
            #         ).execute()
            #         logging.info(f"Added video to playlist: {playlist_id}")
            #     except Exception as e:
            #         logging.error(f"Failed to add video to playlist: {str(e)}")
            
            return True
        
        except HttpError as e:
            logging.error(f"YouTube API HTTP error (attempt {attempt+1}/{max_retries}): {e.resp.status}")
            error_content = json.loads(e.content.decode('utf-8'))
            
            if 'error' in error_content and 'errors' in error_content['error']:
                for error in error_content['error']['errors']:
                    logging.error(f"Error reason: {error.get('reason')}")
                    logging.error(f"Error message: {error.get('message')}")
            
            # Handle quota errors specially
            if e.resp.status == 403 and 'quotaExceeded' in str(e.content):
                logging.error("YouTube quota exceeded. Aborting retry attempts.")
                return False
                
            time.sleep(5)  # Wait before retry
            
        except Exception as e:
            logging.error(f"Unexpected error during YouTube upload (attempt {attempt+1}/{max_retries}): {str(e)}")
            time.sleep(5)  # Wait before retry
    
    logging.error("All YouTube upload attempts failed")
    return False


def upload_to_tiktok(
    video_path: str, 
    caption: str, 
    config: Dict,
    max_retries: int = 3
) -> bool:
    """
    Uploads a video to TikTok using available APIs or unofficial methods.
    Implements multiple approaches with fallbacks.
    
    Args:
        video_path (str): Path to the video file to upload
        caption (str): Video caption including hashtags
        config (dict): Application configuration
        max_retries (int): Maximum number of retry attempts
        
    Returns:
        bool: True if upload was successful, False otherwise
    """
    logging.info(f"Uploading video to TikTok with caption: {caption[:50]}...")
    
    # Validate inputs
    if not video_path or not os.path.exists(video_path):
        logging.error(f"Video file not found: {video_path}")
        return False
    
    if not caption:
        logging.warning("No caption provided for TikTok upload")
        caption = "#AI #Surreal #AutoMagic"
    
    # Check if we have TikTok credentials
    if not config.get("tiktok_session_id"):
        logging.error("TikTok session ID not found in configuration")
        logging.warning("TikTok upload requires valid session cookies")
        return False
    
    # Attempt upload with retry logic and multiple methods
    for attempt in range(max_retries):
        # Try different methods depending on available libraries and APIs
        
        # Method 1: Try direct API upload if we have an API key
        if config.get("tiktok_api_key"):
            try:
                logging.info("Attempting TikTok upload via direct API")
                
                # This is a simplified implementation
                # Official TikTok API access is limited, so this is more theoretical
                headers = {
                    "Authorization": f"Bearer {config['tiktok_api_key']}",
                }
                
                # Example API endpoint (may not be accurate)
                upload_url = "https://open-api.tiktok.com/video/upload/"
                
                with open(video_path, "rb") as video_file:
                    files = {"video": video_file}
                    data = {"caption": caption}
                    
                    response = requests.post(
                        upload_url,
                        headers=headers,
                        files=files,
                        data=data,
                        timeout=60
                    )
                    
                    if response.status_code == 200:
                        result = response.json()
                        if result.get("success"):